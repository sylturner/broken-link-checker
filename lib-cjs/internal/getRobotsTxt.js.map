{"version":3,"sources":["../../lib/internal/getRobotsTxt.js"],"names":["url","auth","cache","options","isURL","lenient","TypeError","BLC_INVALID","URL","hash","pathname","search","stream","GET_METHOD"],"mappings":";;;;;;;AAAA;;AACA;;AACA;;AACA;;AACA;;AACA;;;;AAIA;;;;;;;;;eASe,OAAOA,GAAP,EAAYC,IAAZ,EAAkBC,KAAlB,EAAyBC,OAAzB,KACf;AACC,MAAI,CAACC,eAAMC,OAAN,CAAcL,GAAd,CAAL,EACA;AACC,UAAM,IAAIM,SAAJ,CAAcC,oBAAd,CAAN;AACA,GAHD,MAKA;AACCP,IAAAA,GAAG,GAAG,IAAIQ,GAAJ,CAAQR,GAAR,CAAN;AACAA,IAAAA,GAAG,CAACS,IAAJ,GAAW,EAAX;AACAT,IAAAA,GAAG,CAACU,QAAJ,GAAe,aAAf;AACAV,IAAAA,GAAG,CAACW,MAAJ,GAAa,EAAb;AAEA,UAAM;AAACC,MAAAA;AAAD,QAAW,MAAM,0BAAYZ,GAAZ,EAAiBC,IAAjB,EAAuBY,mBAAvB,EAAmCX,KAAnC,EAA0CC,OAA1C,CAAvB,CAND,CAQC;;AACA,WAAO,6BAAM,MAAM,6BAAMS,MAAN,CAAZ,CAAP;AACA;AACD,C","sourcesContent":["import {BLC_INVALID} from \"./reasons\";\nimport {GET_METHOD} from \"./methods\";\nimport guard from \"robots-txt-guard\";\nimport isURL from \"isurl\";\nimport parse from \"robots-txt-parse\";\nimport requestHTTP from \"./requestHTTP\";\n\n\n\n/**\n * Download and parse a robots.txt file from a server's root path.\n * @param {URL} url\n * @param {object} auth\n * @param {URLCache} cache\n * @param {object} options\n * @throws {TypeError} non-URL\n * @returns {Promise<guard>}\n */\nexport default async (url, auth, cache, options) =>\n{\n\tif (!isURL.lenient(url))\n\t{\n\t\tthrow new TypeError(BLC_INVALID);\n\t}\n\telse\n\t{\n\t\turl = new URL(url);\n\t\turl.hash = \"\";\n\t\turl.pathname = \"/robots.txt\";\n\t\turl.search = \"\";\n\n\t\tconst {stream} = await requestHTTP(url, auth, GET_METHOD, cache, options);\n\n\t\t// @todo https://github.com/tc39/proposal-pipeline-operator\n\t\treturn guard(await parse(stream));\n\t}\n};\n"],"file":"getRobotsTxt.js"}